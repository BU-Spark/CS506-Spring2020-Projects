{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging subfolder of a particular year. Run this until all files of a year are merged into final output file(output_2018)\n",
    "with open('output_2018_1.txt',encoding=\"utf-8\") as fp: \n",
    "    data = fp.read() \n",
    "    \n",
    "with open('output_2018.txt',encoding=\"utf-8\") as fp: \n",
    "    data2 = fp.read() \n",
    "    \n",
    "data += \"\\n\"\n",
    "data += data2 \n",
    "\n",
    "with open ('output_2018.txt', 'w',encoding=\"utf-8\") as fp: \n",
    "    fp.write(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = open(\"output_2018.txt\", \"r\",encoding=\"utf-8\").readlines()\n",
    "for i in range(len(data_df)):\n",
    "    data_df[i] = str(data_df[i]).replace('[','').replace(']','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of names of person murdered in 2018. Created after running \"homicide articles by name\"\n",
    "name_list_df = pd.read_csv('Name_List_2018.csv',encoding=\"utf-8\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering all people of white community. Same code can be used for black community after changing race values to black.\n",
    "name_list=[]\n",
    "for i in range(len(name_list_df)):\n",
    "    if ((name_list_df['Race'][i] in ('W', 'W H','W N','WH','H')) and name_list_df['Number of Articles'][i]!= 0)  :\n",
    "        name_list.append(name_list_df['Full Name'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Colon']\n"
     ]
    }
   ],
   "source": [
    "print(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Colon\n",
      "[(11356100181062323261, 23, 25)]\n"
     ]
    }
   ],
   "source": [
    "#Finding the articles where these names are mentioned in the homicide subset dataset. Storing articles in data_df_name.\n",
    "count = 0\n",
    "count_F=[]\n",
    "data_df_name =[]\n",
    "for i in range(len(name_list)):\n",
    "    matcher.add(\"Phrase Matching\", None, nlp(name_list[i]))\n",
    "    print(nlp(name_list[i]))\n",
    "    for j in range(len(data_df)):\n",
    "        if(count < articles_count[i]):\n",
    "            doc = nlp(data_df[j])\n",
    "            matches = matcher(doc)\n",
    "            #print(len(matches))\n",
    "            if (len(matches) > 0):\n",
    "                print(matches)\n",
    "                count= count + 1\n",
    "                data_df_name.append(data_df[j])\n",
    "        else:\n",
    "            break\n",
    "    matcher.remove(\"Phrase Matching\")\n",
    "    count_F.append(count)\n",
    "    count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the articles mentioning name explicitly in a txt file. \n",
    "#This code is required if you want to merge all the data of specific community of complete 5 years.\n",
    "#and then do topic modelling of complete 5 years together. To do that you will run above code cells again for all the years.\n",
    "#and create separate txt for every year specific to community and then once 5 files are created, merge them and \n",
    "#Then run below topic modeling code. If you don't want to merge all years skip this and next cell. \n",
    "#Start with Topic modelling of particular year.\n",
    "\n",
    "filename = 'homicide_white_data_2018.txt'\n",
    "resultFyle = open(filename,'w',encoding=\"utf-8\")\n",
    "for r in data_df_name:\n",
    "    resultFyle.write(r)\n",
    "resultFyle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to write txt content created after merging all files into one txt file of 5 years into data frame.\n",
    "data_df_name = open(\"homicide_white_complete_data.txt\", \"r\",encoding=\"utf-8\").readlines()\n",
    "for i in range(len(data_df)):\n",
    "    data_df[i] = str(data_df_name[i]).replace('[','').replace(']','')\n",
    "    data_df[i] = str(data_df_name[i]).replace('\\'','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(dict.fromkeys(data_df_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come','said','tell'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "    \n",
    "data_words = list(sent_to_words(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "# or do\n",
    "# !conda install -c conda-forge spacy-model-en_core_web_md \n",
    "# and use nlp=spacy.load('en_core_web_sm') instead in below function.\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.034*\"colon\" + 0.034*\"police\" + 0.034*\"tip\" + 0.033*\"resident\" + 0.033*\"die\" + 0.033*\"identify\" + 0.033*\"local\" + 0.033*\"incident\" + 0.033*\"wish\" + 0.033*\"call\"'), (1, '0.052*\"tip\" + 0.052*\"police\" + 0.052*\"colon\" + 0.031*\"gunshot\" + 0.031*\"anonymously\" + 0.031*\"multiple\" + 0.031*\"wound\" + 0.031*\"crimestopper\" + 0.031*\"year\" + 0.031*\"statement\"')]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only works when number of topics are 4 in above cell(num_topics)\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.colors as mcolors\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 7500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=10)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count for White Community', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis\n",
    "#pyLDAvis.save_html(vis, 'output_2018_white.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
