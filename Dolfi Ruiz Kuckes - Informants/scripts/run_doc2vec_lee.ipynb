{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Doc2Vec Model\n",
    "=============\n",
    "\n",
    "Introduces Gensim's Doc2Vec model and demonstrates its use on the Lee Corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a `core_concepts_model` that represents each\n",
    "`core_concepts_document` as a `core_concepts_vector`.  This\n",
    "tutorial introduces the model and demonstrates how to train and assess it.\n",
    "\n",
    "Here's a list of what we'll be doing:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "Review: Bag-of-words\n",
    "--------------------\n",
    "\n",
    ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
    "\n",
    "You may be familiar with the `bag-of-words model\n",
    "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
    "`core_concepts_vector` section.\n",
    "This model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order: \"John likes Mary\" and\n",
    "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
    "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
    "models consider word phrases of length n to represent documents as\n",
    "fixed-length vectors to capture local word order but suffer from data\n",
    "sparsity and high dimensionality.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "Review: ``Word2Vec`` Model\n",
    "--------------------------\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
    "\n",
    "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
    "But what if we want to calculate a vector for the **entire document**\\ ?\n",
    "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
    "However, there is a better way...\n",
    "\n",
    "Introducing: Paragraph Vector\n",
    "-----------------------------\n",
    "\n",
    ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
    "\n",
    "Le and Mikolov in 2014 introduced the `Doc2Vec algorithm <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__, which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like\n",
    "vector, which contributes to all training predictions, and is updated like\n",
    "other word-vectors, but we will call it a doc-vector. Gensim's\n",
    ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
    "\n",
    "There are two implementations:\n",
    "\n",
    "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    ".. Important::\n",
    "  Don't let the implementation details below scare you.\n",
    "  They're advanced material: if it's too much, then move on to the next section.\n",
    "\n",
    "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a center word based an\n",
    "average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a target word just from\n",
    "the full document's doc-vector. (It is also common to combine this with\n",
    "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
    "predict a single target word, but only one at a time.)\n",
    "\n",
    "Prepare the Training and Test Data\n",
    "----------------------------------\n",
    "\n",
    "For this tutorial, we'll be training our model using the `Lee Background\n",
    "Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "included in gensim. This corpus contains 314 documents selected from the\n",
    "Australian Broadcasting Corporationâ€™s news mail service, which provides text\n",
    "e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter `Lee Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "which contains 50 documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 11:28:02,776 : WARNING : unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Function to Read and Preprocess Text\n",
    "---------------------------------------------\n",
    "\n",
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "- read the file line-by-line\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file we're reading is a **corpus**.\n",
    "Each line of the file is a **document**.\n",
    "\n",
    ".. Important::\n",
    "  To train the model, we'll need to associate a tag/number with each document\n",
    "  of the training corpus. In our case, the tag is simply the zero-based line\n",
    "  number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "imported utility.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utility\n",
    "all_state, all_sjc = utility.combine_cases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>headnote</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMMONWEALTH  vs.  ADMILSON RESENDE.</td>\n",
       "      <td>Controlled Substances. Constitutional Law, Ple...</td>\n",
       "      <td>\\nThe present case is the most recent in a ser...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COMMONWEALTH  vs.  GEORGE PHILBROOK.</td>\n",
       "      <td>Homicide. Evidence, Prior violent conduct, Sta...</td>\n",
       "      <td>\\nThe defendant was convicted of murder in the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LINDA S. BOWERS  vs.  P. WILE'S, INC.</td>\n",
       "      <td>Negligence, Retailer. Notice. Practice, Civil,...</td>\n",
       "      <td>\\nIn this case we are called upon to determine...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COMMONWEALTH  vs.  JARED ABDALLAH.</td>\n",
       "      <td>Constitutional Law, Search and seizure. Search...</td>\n",
       "      <td>\\nAfter causing a disturbance, the defendant w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COMMONWEALTH  vs.  ROBERT D. WADE.</td>\n",
       "      <td>Amended October 28, 2016\\nDeoxyribonucleic Aci...</td>\n",
       "      <td>\\nThis case requires us to decide whether the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>IN THE MATTER OF CLAUDE DAVID</td>\n",
       "      <td>SJC-12642\\nAttorney at Law, Disciplinary proce...</td>\n",
       "      <td>The respondent, Claude David Grayer, appeals f...</td>\n",
       "      <td>December 2, 2019</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>KAROL E. SIMONTON, pet</td>\n",
       "      <td>SJC-12588\\nHabeas Corpus. Practice, Criminal, ...</td>\n",
       "      <td>Karol E. Simonton appeals from a judgment of t...</td>\n",
       "      <td>December 11, 2019</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>A.F.</td>\n",
       "      <td>SJC-12686\\nHarassment Prevention. Supreme Judi...</td>\n",
       "      <td>The petitioner, A.F., appeals from a judgment ...</td>\n",
       "      <td>December 11, 2019</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>IN THE MATTER OF CARL MARTIN</td>\n",
       "      <td>SJC-12589\\nAttorney at Law, Admission to pract...</td>\n",
       "      <td>Carl Martin Swanson has filed, in the county c...</td>\n",
       "      <td>December 12, 2019</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>BHARANIDHARAN PADMANABHAN vs. LORETT</td>\n",
       "      <td>SJC-12718\\nPractice, Civil, Action in nature o...</td>\n",
       "      <td>The petitioner, Bharanidharan Padmanabhan, app...</td>\n",
       "      <td>December 12, 2019</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2655 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         case  \\\n",
       "0        COMMONWEALTH  vs.  ADMILSON RESENDE.   \n",
       "1        COMMONWEALTH  vs.  GEORGE PHILBROOK.   \n",
       "2      LINDA S. BOWERS  vs.  P. WILE'S, INC.    \n",
       "3          COMMONWEALTH  vs.  JARED ABDALLAH.   \n",
       "4          COMMONWEALTH  vs.  ROBERT D. WADE.   \n",
       "...                                       ...   \n",
       "2004            IN THE MATTER OF CLAUDE DAVID   \n",
       "2005                   KAROL E. SIMONTON, pet   \n",
       "2006                                    A.F.    \n",
       "2007            IN THE MATTER OF CARL MARTIN    \n",
       "2008     BHARANIDHARAN PADMANABHAN vs. LORETT   \n",
       "\n",
       "                                               headnote  \\\n",
       "0     Controlled Substances. Constitutional Law, Ple...   \n",
       "1     Homicide. Evidence, Prior violent conduct, Sta...   \n",
       "2     Negligence, Retailer. Notice. Practice, Civil,...   \n",
       "3     Constitutional Law, Search and seizure. Search...   \n",
       "4     Amended October 28, 2016\\nDeoxyribonucleic Aci...   \n",
       "...                                                 ...   \n",
       "2004  SJC-12642\\nAttorney at Law, Disciplinary proce...   \n",
       "2005  SJC-12588\\nHabeas Corpus. Practice, Criminal, ...   \n",
       "2006  SJC-12686\\nHarassment Prevention. Supreme Judi...   \n",
       "2007  SJC-12589\\nAttorney at Law, Admission to pract...   \n",
       "2008  SJC-12718\\nPractice, Civil, Action in nature o...   \n",
       "\n",
       "                                                   text               date  \\\n",
       "0     \\nThe present case is the most recent in a ser...                NaN   \n",
       "1     \\nThe defendant was convicted of murder in the...                NaN   \n",
       "2     \\nIn this case we are called upon to determine...                NaN   \n",
       "3     \\nAfter causing a disturbance, the defendant w...                NaN   \n",
       "4     \\nThis case requires us to decide whether the ...                NaN   \n",
       "...                                                 ...                ...   \n",
       "2004  The respondent, Claude David Grayer, appeals f...   December 2, 2019   \n",
       "2005  Karol E. Simonton appeals from a judgment of t...  December 11, 2019   \n",
       "2006  The petitioner, A.F., appeals from a judgment ...  December 11, 2019   \n",
       "2007  Carl Martin Swanson has filed, in the county c...  December 12, 2019   \n",
       "2008  The petitioner, Bharanidharan Padmanabhan, app...  December 12, 2019   \n",
       "\n",
       "     county  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "...     ...  \n",
       "2004         \n",
       "2005         \n",
       "2006         \n",
       "2007         \n",
       "2008         \n",
       "\n",
       "[2655 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['argentine', 'president', 'adolfo', 'rodriguez', 'saa', 'has', 'asked', 'the', 'country', 'banks', 'to', 'help', 're', 'establish', 'peace', 'by', 'facilitating', 'the', 'payment', 'of', 'pensions', 'and', 'salaries', 'to', 'workers', 'and', 'retirees', 'he', 'says', 'he', 'issued', 'the', 'appeal', 'at', 'meeting', 'with', 'leaders', 'of', 'the', 'banking', 'community', 'very', 'concerned', 'about', 'what', 'has', 'happened', 'in', 'argentina', 'mr', 'rodriguez', 'saa', 'said', 'he', 'says', 'he', 'has', 'asked', 'banks', 'to', 'remain', 'open', 'from', 'am', 'to', 'pm', 'monday', 'to', 'be', 'able', 'to', 'cash', 'checks', 'of', 'up', 'to', 'pesos', 'or', 'us', 'per', 'person'], [20])\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain\n",
    "any tags.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n",
    "iterating over the training corpus 40 times. We set the minimum word count to\n",
    "2 in order to discard words with very few occurrences. (Without a variety of\n",
    "representative examples, retaining such infrequent words can often make a\n",
    "model worse!) Typical iteration counts in the published `Paragraph Vector paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__\n",
    "results, using 10s-of-thousands to millions of docs, are 10-20. More\n",
    "iterations take more time and eventually reach a point of diminishing\n",
    "returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish\n",
    "documents (a few hundred words). Adding training passes can sometimes help\n",
    "with such small datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 10:55:33,700 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 10:55:42,753 : INFO : collecting all words and their counts\n",
      "2020-05-01 10:55:42,754 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-05-01 10:55:42,768 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "2020-05-01 10:55:42,769 : INFO : Loading a fresh vocabulary\n",
      "2020-05-01 10:55:42,780 : INFO : effective_min_count=2 retains 3955 unique words (56% of original 6981, drops 3026)\n",
      "2020-05-01 10:55:42,781 : INFO : effective_min_count=2 leaves 55126 word corpus (94% of original 58152, drops 3026)\n",
      "2020-05-01 10:55:42,795 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2020-05-01 10:55:42,796 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2020-05-01 10:55:42,796 : INFO : downsampling leaves estimated 42390 word corpus (76.9% of prior 55126)\n",
      "2020-05-01 10:55:42,809 : INFO : estimated required memory for 3955 words and 50 dimensions: 3619500 bytes\n",
      "2020-05-01 10:55:42,810 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via\n",
    "``model.wv.vocab``\\ ) of all of the unique words extracted from the training\n",
    "corpus along with the count (e.g., ``model.wv.vocab['penalty'].count`` for\n",
    "counts for the word ``penalty``\\ ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model on the corpus.\n",
    "If the BLAS library is being used, this should take no more than 3 seconds.\n",
    "If the BLAS library is not being used, this should take no more than 2\n",
    "minutes, so use BLAS if you value your time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 10:55:48,528 : INFO : training model with 3 workers on 3955 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-05-01 10:55:55,018 : INFO : EPOCH 1 - PROGRESS: at 16.67% examples, 1093 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:56:01,292 : INFO : EPOCH 1 - PROGRESS: at 66.67% examples, 2241 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:56:01,293 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:56:01,668 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:56:01,699 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:56:01,700 : INFO : EPOCH - 1 : training on 58152 raw words (42508 effective words) took 13.2s, 3228 effective words/s\n",
      "2020-05-01 10:56:07,931 : INFO : EPOCH 2 - PROGRESS: at 16.67% examples, 1128 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:56:14,217 : INFO : EPOCH 2 - PROGRESS: at 66.67% examples, 2275 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:56:14,218 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:56:14,523 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:56:14,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:56:14,584 : INFO : EPOCH - 2 : training on 58152 raw words (42336 effective words) took 12.9s, 3287 effective words/s\n",
      "2020-05-01 10:56:21,044 : INFO : EPOCH 3 - PROGRESS: at 16.67% examples, 1092 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:56:27,315 : INFO : EPOCH 3 - PROGRESS: at 66.67% examples, 2243 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:56:27,316 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:56:27,487 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:56:27,547 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:56:27,548 : INFO : EPOCH - 3 : training on 58152 raw words (42377 effective words) took 13.0s, 3270 effective words/s\n",
      "2020-05-01 10:56:33,859 : INFO : EPOCH 4 - PROGRESS: at 18.67% examples, 1136 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:56:40,280 : INFO : EPOCH 4 - PROGRESS: at 66.67% examples, 2240 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:56:40,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:56:40,396 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:56:40,415 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:56:40,416 : INFO : EPOCH - 4 : training on 58152 raw words (42286 effective words) took 12.9s, 3287 effective words/s\n",
      "2020-05-01 10:56:46,884 : INFO : EPOCH 5 - PROGRESS: at 16.67% examples, 1088 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:56:53,213 : INFO : EPOCH 5 - PROGRESS: at 66.67% examples, 2222 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:56:53,214 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:56:53,448 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:56:53,454 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:56:53,454 : INFO : EPOCH - 5 : training on 58152 raw words (42288 effective words) took 13.0s, 3244 effective words/s\n",
      "2020-05-01 10:56:59,781 : INFO : EPOCH 6 - PROGRESS: at 16.67% examples, 1114 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:57:06,198 : INFO : EPOCH 6 - PROGRESS: at 66.67% examples, 2235 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:57:06,199 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:57:06,378 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:57:06,471 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:57:06,472 : INFO : EPOCH - 6 : training on 58152 raw words (42440 effective words) took 13.0s, 3261 effective words/s\n",
      "2020-05-01 10:57:12,962 : INFO : EPOCH 7 - PROGRESS: at 16.67% examples, 1083 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:57:19,386 : INFO : EPOCH 7 - PROGRESS: at 66.67% examples, 2206 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:57:19,387 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:57:19,421 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:57:19,471 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:57:19,472 : INFO : EPOCH - 7 : training on 58152 raw words (42385 effective words) took 13.0s, 3262 effective words/s\n",
      "2020-05-01 10:57:25,673 : INFO : EPOCH 8 - PROGRESS: at 16.67% examples, 1133 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:57:31,899 : INFO : EPOCH 8 - PROGRESS: at 66.67% examples, 2298 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:57:31,900 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:57:32,229 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:57:32,230 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:57:32,230 : INFO : EPOCH - 8 : training on 58152 raw words (42404 effective words) took 12.8s, 3324 effective words/s\n",
      "2020-05-01 10:57:39,276 : INFO : EPOCH 9 - PROGRESS: at 16.67% examples, 999 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:57:45,732 : INFO : EPOCH 9 - PROGRESS: at 66.67% examples, 2107 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:57:45,733 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:57:45,836 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:57:45,896 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:57:45,897 : INFO : EPOCH - 9 : training on 58152 raw words (42325 effective words) took 13.7s, 3097 effective words/s\n",
      "2020-05-01 10:57:52,640 : INFO : EPOCH 10 - PROGRESS: at 16.67% examples, 1048 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:57:58,861 : INFO : EPOCH 10 - PROGRESS: at 66.67% examples, 2203 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:57:58,862 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:57:58,994 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:57:59,023 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:57:59,023 : INFO : EPOCH - 10 : training on 58152 raw words (42382 effective words) took 13.1s, 3230 effective words/s\n",
      "2020-05-01 10:58:05,425 : INFO : EPOCH 11 - PROGRESS: at 16.67% examples, 1108 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:58:11,730 : INFO : EPOCH 11 - PROGRESS: at 66.67% examples, 2250 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:58:11,731 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:58:11,868 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:58:11,880 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:58:11,880 : INFO : EPOCH - 11 : training on 58152 raw words (42480 effective words) took 12.9s, 3305 effective words/s\n",
      "2020-05-01 10:58:18,207 : INFO : EPOCH 12 - PROGRESS: at 16.67% examples, 1117 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:58:24,395 : INFO : EPOCH 12 - PROGRESS: at 66.67% examples, 2286 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:58:24,396 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:58:24,583 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:58:24,588 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:58:24,589 : INFO : EPOCH - 12 : training on 58152 raw words (42478 effective words) took 12.7s, 3344 effective words/s\n",
      "2020-05-01 10:58:30,880 : INFO : EPOCH 13 - PROGRESS: at 16.67% examples, 1128 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:58:37,009 : INFO : EPOCH 13 - PROGRESS: at 66.67% examples, 2297 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:58:37,011 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:58:37,137 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:58:37,160 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:58:37,161 : INFO : EPOCH - 13 : training on 58152 raw words (42290 effective words) took 12.6s, 3365 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 10:58:43,267 : INFO : EPOCH 14 - PROGRESS: at 16.67% examples, 1153 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:58:49,218 : INFO : EPOCH 14 - PROGRESS: at 66.67% examples, 2363 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:58:49,219 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:58:49,609 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:58:49,617 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:58:49,618 : INFO : EPOCH - 14 : training on 58152 raw words (42392 effective words) took 12.5s, 3404 effective words/s\n",
      "2020-05-01 10:58:55,707 : INFO : EPOCH 15 - PROGRESS: at 16.67% examples, 1154 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:59:01,839 : INFO : EPOCH 15 - PROGRESS: at 66.67% examples, 2324 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:59:01,840 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:59:02,075 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:59:02,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:59:02,101 : INFO : EPOCH - 15 : training on 58152 raw words (42260 effective words) took 12.5s, 3386 effective words/s\n",
      "2020-05-01 10:59:08,059 : INFO : EPOCH 16 - PROGRESS: at 16.67% examples, 1184 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:59:13,927 : INFO : EPOCH 16 - PROGRESS: at 66.67% examples, 2417 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:59:13,928 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:59:14,558 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:59:14,561 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:59:14,562 : INFO : EPOCH - 16 : training on 58152 raw words (42513 effective words) took 12.5s, 3412 effective words/s\n",
      "2020-05-01 10:59:20,684 : INFO : EPOCH 17 - PROGRESS: at 16.67% examples, 1155 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:59:26,666 : INFO : EPOCH 17 - PROGRESS: at 66.67% examples, 2359 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:59:26,667 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:59:27,005 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:59:27,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:59:27,036 : INFO : EPOCH - 17 : training on 58152 raw words (42444 effective words) took 12.5s, 3403 effective words/s\n",
      "2020-05-01 10:59:33,485 : INFO : EPOCH 18 - PROGRESS: at 15.33% examples, 1136 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:59:39,636 : INFO : EPOCH 18 - PROGRESS: at 66.67% examples, 2267 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:59:39,637 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:59:39,731 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:59:39,777 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:59:39,777 : INFO : EPOCH - 18 : training on 58152 raw words (42393 effective words) took 12.7s, 3328 effective words/s\n",
      "2020-05-01 10:59:45,678 : INFO : EPOCH 19 - PROGRESS: at 15.33% examples, 1243 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 10:59:51,342 : INFO : EPOCH 19 - PROGRESS: at 66.67% examples, 2471 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 10:59:51,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 10:59:52,149 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 10:59:52,187 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 10:59:52,187 : INFO : EPOCH - 19 : training on 58152 raw words (42474 effective words) took 12.4s, 3424 effective words/s\n",
      "2020-05-01 10:59:58,549 : INFO : EPOCH 20 - PROGRESS: at 16.67% examples, 1114 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:00:04,814 : INFO : EPOCH 20 - PROGRESS: at 66.67% examples, 2260 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:00:04,817 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:00:04,897 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:00:04,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:00:04,908 : INFO : EPOCH - 20 : training on 58152 raw words (42364 effective words) took 12.7s, 3331 effective words/s\n",
      "2020-05-01 11:00:11,197 : INFO : EPOCH 21 - PROGRESS: at 16.67% examples, 1123 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:00:17,355 : INFO : EPOCH 21 - PROGRESS: at 66.67% examples, 2298 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:00:17,356 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:00:17,579 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:00:17,590 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:00:17,590 : INFO : EPOCH - 21 : training on 58152 raw words (42468 effective words) took 12.7s, 3350 effective words/s\n",
      "2020-05-01 11:00:23,904 : INFO : EPOCH 22 - PROGRESS: at 16.67% examples, 1105 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:00:30,106 : INFO : EPOCH 22 - PROGRESS: at 67.00% examples, 2267 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:00:30,107 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:00:30,142 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:00:30,158 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:00:30,158 : INFO : EPOCH - 22 : training on 58152 raw words (42436 effective words) took 12.6s, 3377 effective words/s\n",
      "2020-05-01 11:00:36,297 : INFO : EPOCH 23 - PROGRESS: at 16.67% examples, 1147 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:00:42,329 : INFO : EPOCH 23 - PROGRESS: at 66.67% examples, 2343 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:00:42,330 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:00:42,645 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:00:42,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:00:42,668 : INFO : EPOCH - 23 : training on 58152 raw words (42363 effective words) took 12.5s, 3388 effective words/s\n",
      "2020-05-01 11:00:48,841 : INFO : EPOCH 24 - PROGRESS: at 16.67% examples, 1142 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:00:54,970 : INFO : EPOCH 24 - PROGRESS: at 66.67% examples, 2317 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:00:54,971 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:00:55,133 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:00:55,165 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:00:55,165 : INFO : EPOCH - 24 : training on 58152 raw words (42303 effective words) took 12.5s, 3385 effective words/s\n",
      "2020-05-01 11:01:01,817 : INFO : EPOCH 25 - PROGRESS: at 16.67% examples, 1059 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:01:08,051 : INFO : EPOCH 25 - PROGRESS: at 67.67% examples, 2218 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:01:08,053 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:01:08,094 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:01:08,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:01:08,116 : INFO : EPOCH - 25 : training on 58152 raw words (42384 effective words) took 12.9s, 3273 effective words/s\n",
      "2020-05-01 11:01:14,002 : INFO : EPOCH 26 - PROGRESS: at 16.67% examples, 1199 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:01:19,769 : INFO : EPOCH 26 - PROGRESS: at 66.67% examples, 2450 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:01:19,770 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:01:20,437 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:01:20,510 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:01:20,511 : INFO : EPOCH - 26 : training on 58152 raw words (42390 effective words) took 12.4s, 3421 effective words/s\n",
      "2020-05-01 11:01:26,775 : INFO : EPOCH 27 - PROGRESS: at 16.67% examples, 1122 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 11:01:32,953 : INFO : EPOCH 27 - PROGRESS: at 66.67% examples, 2286 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:01:32,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:01:32,990 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:01:33,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:01:33,011 : INFO : EPOCH - 27 : training on 58152 raw words (42311 effective words) took 12.5s, 3385 effective words/s\n",
      "2020-05-01 11:01:39,361 : INFO : EPOCH 28 - PROGRESS: at 15.33% examples, 1152 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:01:45,423 : INFO : EPOCH 28 - PROGRESS: at 66.67% examples, 2294 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:01:45,424 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:01:45,544 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:01:45,572 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:01:45,572 : INFO : EPOCH - 28 : training on 58152 raw words (42387 effective words) took 12.6s, 3375 effective words/s\n",
      "2020-05-01 11:01:51,785 : INFO : EPOCH 29 - PROGRESS: at 16.67% examples, 1139 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:01:57,828 : INFO : EPOCH 29 - PROGRESS: at 66.67% examples, 2327 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:01:57,829 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:01:58,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:01:58,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:01:58,035 : INFO : EPOCH - 29 : training on 58152 raw words (42341 effective words) took 12.5s, 3398 effective words/s\n",
      "2020-05-01 11:02:04,170 : INFO : EPOCH 30 - PROGRESS: at 16.67% examples, 1148 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:02:10,207 : INFO : EPOCH 30 - PROGRESS: at 66.67% examples, 2342 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:02:10,208 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:02:10,567 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:02:10,569 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:02:10,570 : INFO : EPOCH - 30 : training on 58152 raw words (42340 effective words) took 12.5s, 3378 effective words/s\n",
      "2020-05-01 11:02:16,542 : INFO : EPOCH 31 - PROGRESS: at 16.67% examples, 1180 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:02:22,501 : INFO : EPOCH 31 - PROGRESS: at 66.67% examples, 2391 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:02:22,502 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:02:23,056 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:02:23,083 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:02:23,083 : INFO : EPOCH - 31 : training on 58152 raw words (42340 effective words) took 12.5s, 3384 effective words/s\n",
      "2020-05-01 11:02:29,125 : INFO : EPOCH 32 - PROGRESS: at 18.67% examples, 1189 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:02:35,114 : INFO : EPOCH 32 - PROGRESS: at 66.67% examples, 2371 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:02:35,114 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:02:35,555 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:02:35,578 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:02:35,578 : INFO : EPOCH - 32 : training on 58152 raw words (42415 effective words) took 12.5s, 3395 effective words/s\n",
      "2020-05-01 11:02:41,855 : INFO : EPOCH 33 - PROGRESS: at 16.67% examples, 1120 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:02:48,038 : INFO : EPOCH 33 - PROGRESS: at 67.67% examples, 2302 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:02:48,039 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:02:48,105 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:02:48,128 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:02:48,129 : INFO : EPOCH - 33 : training on 58152 raw words (42487 effective words) took 12.5s, 3386 effective words/s\n",
      "2020-05-01 11:02:54,426 : INFO : EPOCH 34 - PROGRESS: at 16.67% examples, 1114 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:03:00,609 : INFO : EPOCH 34 - PROGRESS: at 66.67% examples, 2284 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:03:00,610 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:03:00,674 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:03:00,742 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:03:00,742 : INFO : EPOCH - 34 : training on 58152 raw words (42407 effective words) took 12.6s, 3364 effective words/s\n",
      "2020-05-01 11:03:06,779 : INFO : EPOCH 35 - PROGRESS: at 16.67% examples, 1170 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:03:12,770 : INFO : EPOCH 35 - PROGRESS: at 66.67% examples, 2373 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:03:12,771 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:03:13,111 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:03:13,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:03:13,185 : INFO : EPOCH - 35 : training on 58152 raw words (42431 effective words) took 12.4s, 3411 effective words/s\n",
      "2020-05-01 11:03:19,439 : INFO : EPOCH 36 - PROGRESS: at 16.67% examples, 1126 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:03:25,596 : INFO : EPOCH 36 - PROGRESS: at 66.67% examples, 2292 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:03:25,597 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:03:25,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:03:25,699 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:03:25,699 : INFO : EPOCH - 36 : training on 58152 raw words (42291 effective words) took 12.5s, 3381 effective words/s\n",
      "2020-05-01 11:03:31,965 : INFO : EPOCH 37 - PROGRESS: at 16.67% examples, 1127 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:03:38,630 : INFO : EPOCH 37 - PROGRESS: at 66.67% examples, 2209 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:03:38,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:03:38,849 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:03:38,870 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:03:38,871 : INFO : EPOCH - 37 : training on 58152 raw words (42448 effective words) took 13.2s, 3224 effective words/s\n",
      "2020-05-01 11:03:45,154 : INFO : EPOCH 38 - PROGRESS: at 16.67% examples, 1130 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:03:51,336 : INFO : EPOCH 38 - PROGRESS: at 66.67% examples, 2296 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:03:51,338 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:03:51,459 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:03:51,466 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:03:51,467 : INFO : EPOCH - 38 : training on 58152 raw words (42507 effective words) took 12.6s, 3375 effective words/s\n",
      "2020-05-01 11:03:57,540 : INFO : EPOCH 39 - PROGRESS: at 16.67% examples, 1167 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:04:03,710 : INFO : EPOCH 39 - PROGRESS: at 66.67% examples, 2332 words/s, in_qsize 2, out_qsize 1\n",
      "2020-05-01 11:04:03,711 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:04:03,966 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:04:03,988 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:04:03,989 : INFO : EPOCH - 39 : training on 58152 raw words (42452 effective words) took 12.5s, 3391 effective words/s\n",
      "2020-05-01 11:04:10,150 : INFO : EPOCH 40 - PROGRESS: at 16.67% examples, 1144 words/s, in_qsize 5, out_qsize 0\n",
      "2020-05-01 11:04:16,212 : INFO : EPOCH 40 - PROGRESS: at 66.67% examples, 2334 words/s, in_qsize 2, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 11:04:16,213 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-01 11:04:16,527 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-01 11:04:16,547 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-01 11:04:16,547 : INFO : EPOCH - 40 : training on 58152 raw words (42379 effective words) took 12.6s, 3376 effective words/s\n",
      "2020-05-01 11:04:16,548 : INFO : training on a 2326080 raw words (1695699 effective words) took 508.0s, 3338 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the trained model to infer a vector for any piece of text\n",
    "by passing a list of words to the ``model.infer_vector`` function. This\n",
    "vector can then be compared with other vectors via cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09087306  0.32701305  0.06860156 -0.05938696  0.11786573 -0.07121419\n",
      " -0.02741257  0.1069333   0.13580915 -0.23466207 -0.01952953 -0.02204405\n",
      " -0.02715763  0.05652005  0.34927163  0.23526803 -0.157106    0.01667246\n",
      "  0.22597577  0.07169604  0.1451198  -0.07471588  0.22620082  0.05322214\n",
      "  0.10549111 -0.05575021  0.0084979   0.04471283 -0.04137658 -0.03507899\n",
      "  0.09665898  0.03951665  0.08097854  0.04166994 -0.24736458 -0.08139656\n",
      " -0.09626713 -0.25195324 -0.25258428 -0.14652404 -0.06479393  0.18179579\n",
      " -0.06570094  0.02638765  0.05731441  0.00684524 -0.12158128 -0.01456059\n",
      "  0.25988027 -0.1881302 ]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
    "string tokens, which should have already been tokenized the same way as the\n",
    "``words`` property of original training document objects.\n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an\n",
    "iterative approximation problem that makes use of internal randomization,\n",
    "repeated inferences of the same text will return slightly different vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the Model\n",
    "-------------------\n",
    "\n",
    "To assess our new model, we'll first infer new vectors for each document of\n",
    "the training corpus, compare the inferred vectors with the training corpus,\n",
    "and then returning the rank of the document based on self-similarity.\n",
    "Basically, we're pretending as if the training corpus is some new unseen data\n",
    "and then seeing how they compare with the trained model. The expectation is\n",
    "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
    "2) and so we should be able to find similar documents very easily.\n",
    "Additionally, we'll keep track of the second ranks for a comparison of less\n",
    "similar documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 11:04:17,961 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus\n",
    "\n",
    "NB. Results vary between runs due to random seeding and very small corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 292, 1: 8})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most\n",
    "similar to itself and about 5% of the time it is mistakenly most similar to\n",
    "another document. Checking the inferred-vector against a\n",
    "training-vector is a sort of 'sanity check' as to whether the model is\n",
    "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (299): Â«australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as wellÂ»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (299, 0.9363336563110352): Â«australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as wellÂ»\n",
      "\n",
      "SECOND-MOST (146, 0.8092724084854126): Â«the australian and south african sides for the first cricket test starting at the adelaide oval today are not expected to be finalised until just before the start of play australian captain steve waugh and his south african counterpart shaun pollock will decide on their lineups after an inspection of the pitch shortly before the start of play the match holds special significance for waugh and his twin brother mark who play their th test together steve waugh is not placing too much relevance on the milestone don want to read too much into it guess and then get too carried away but later on when we retire and look back on it it will be significant it nice for the family mum and dad all the sacrifices they made you know with us growing up and also our brothers so you know it nice for the family he saidÂ»\n",
      "\n",
      "MEDIAN (28, 0.26830142736434937): Â«united states federal magistrate has refused to free on bail british man accused of trying to detonate his explosives laden shoes on transatlantic flight at hearing in united states federal court federal bureau of investigation agent margaret cronin said richard reid was in possession of functioning improvised device which if placed beside an outer wall could have or would have created large hole in the fuselage of the plane the device contained an explosive called tatp us magistrate judith dein refused to grant bail for reid but left open the possibility of his release later reid is charged with intimidation and interfering with flight crew offenses that carry year jail terms no additional charges were filed he allegedly tried to set fire to his sneakers saturday on an american airlines flight from paris to miami that was diverted to boston investigators say the explosives in his shoes were powerful enough to have created major disasterÂ»\n",
      "\n",
      "LEAST (85, -0.06641772389411926): Â«hamas militants have fought gun battles with palestinian security forces in the gaza strip trying to arrest one of the islamic group senior political leaders reports say the fight erupted in the gaza strip after dozens of hamas members surrounded the home of abdel aziz al rantissi when palestinian police arrived to detain him the palestinian leader yasser arafat under international pressure to crack down on militants after wave of suicide bombings in israel in the past month has outlawed the military wings of hamas and other groups and arrested dozens of militantsÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document (usually the same text) is has a\n",
    "similarity score approaching 1.0. However, the similarity score for the\n",
    "second-ranked documents should be significantly lower (assuming the documents\n",
    "are in fact different) and the reasoning becomes obvious when we examine the\n",
    "text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document\n",
    "comparisons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (47): Â«australia will be aiming to take early wickets on day two of the second cricket test against south africa at the mcg the proteas will resume at three for after day one was badly affected by rain with only overs possible australian paceman glenn mcgrath who has two wickets says the catch taken by matthew hayden yesterday is typical of australia outstanding slips fielding this summer in the series so far there been some great catches ricky ponting in the last test occasionally get one myself he said it gives you so much more confidence when you know per cent of the catches that go flying to the slips or through the slips are going to be takenÂ»\n",
      "\n",
      "Similar Document (139, 0.8667811751365662): Â«australia will be looking to score quickly today to set south africa challenging victory target on day four of the first cricket test in adelaide the australians will resume their second innings at for an overall lead of south africa was dismissed late yesterday for with shane warne taking five wickets for the th time in his test career warne says australia is well placed to win was very happy with our position in the match wickets in hand and runs ahead on pitch that deteriorating think much rather be in our shoes than theirs he said but he says australia will need to bat well today south africa can come out and bowl us out for and suddenly they re chasing or we can make to and they need it going to be great last two daysÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: Â«{}Â»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "-----------------\n",
    "\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen\n",
    "test document, and compare the document to our model by eye.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (26): Â«how did allegedly unregistered missile warheads come to be stored on canadian businessman anti terrorism training facility in new mexico and canadian officials are still trying to figure that out but one security expert says the mystery is chilling one david hudak was arrested in the united states more than week ago when according to court documents agents searching his property found the warheads stored in crates that were marked charge demolitionÂ»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (41, 0.6550991535186768): Â«the man accused to trying to blow up an american airlines flight on sunday could not have acted alone according to british islamic leader who knew richard reid well abdul hak baker is the head of the brixton mosque in south london where year old mr reid had worshipped mr baker says mr reid is petty criminal who had converted to islam while in jail he says mr reid had become more and more militant in his outlook after becoming involved with group of muslim extremists the mosque leader says mr reid was easily led and he is not surprised at what happened wouldn say we were totally surprised because we said if we remember how he left us this is what he was believing in mr baker said this was the type of jihad that he was beginning to believe in september the eleventh would have had profound effect on him meanwhile us television is reporting osama bin laden loyalists held prisoner in afghanistan have told us troops mr reid trained in their campsÂ»\n",
      "\n",
      "MEDIAN (240, 0.2217557430267334): Â«counting is proceeding very slowly in the solomon islands national elections as officials are keen to avoid any allegations of vote tampering there was heavy police presence around the three counting centres in the solomon islands capital honiara when the first ballot boxes were opened late last night counting in some of the more remote of the seats in the solomon islands parliament will not even start until saturday after the ballot boxes are transferred to single location large contingent of international observers are monitoring the count in an election that is regarded as crucial if the solomons is to break out of cycle of economic and social disintegration flowing from bitter ethnic war between armed militants from its two largest provincesÂ»\n",
      "\n",
      "LEAST (37, -0.19510991871356964): Â«australia quicks and opening batsmen have put the side in dominant position going into day three of the boxing day test match against south africa at the mcg australia is no wicket for only runs shy of south africa after andy bichel earlier starred as the tourists fell for when play was abandoned due to rain few overs short of scheduled stumps yesterday justin langer was not out and matthew hayden the openers went on the attack from the start with langer innings including six fours and hayden eight earlier shaun pollock and nantie haywood launched vital rearguard action to help south africa to respectable first innings total the pair put on runs for the final wicket to help the tourists to the south africans had slumped to for through combination of australia good bowling good fielding and good luck after resuming at for yesterday morning the tourists looked to be cruising as jacques kallis and neil mckenzie added without loss but then bichel suddenly had them reeling after snatching two wickets in two balls first he had jacques kallis caught behind for although kallis could consider himself very unlucky as replays showed his bat was long way from the ball on the next ball bichel snatched sharp return catch to dismiss lance klusener first ball and have shot at hat trick bichel missed out on the hat trick and mark boucher and neil mckenzie again steadied the south african innings adding before the introduction of part timer mark waugh to the attack paid off for australia waugh removed boucher for caught by bichel brett lee then chipped in trapping mckenzie leg before for with perfect inswinger bichel continued his good day in the field running out claude henderson for with direct hit from the in field lee roared in to allan donald bouncing him and then catching the edge with rising delivery which ricky ponting happily swallowed at third slip to remove the returning paceman for duck bichel did not get his hat trick but ended with the best figures of the australian bowlers after also picking up the final wicket of nantie haywood for lee took for and glenn mcgrath forÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "Let's review what we've seen in this tutorial:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
    "\n",
    "Additional Resources\n",
    "--------------------\n",
    "\n",
    "If you'd like to know more about the subject matter of this tutorial, check out the links below.\n",
    "\n",
    "* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n",
    "* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n",
    "* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n",
    "* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n",
    "* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
