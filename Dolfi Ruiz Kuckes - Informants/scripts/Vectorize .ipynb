{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import json\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielruiz/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the data\n",
    "newest_sjc = \"../data/new data w right fields/cases_mass_new_3_21.json\"\n",
    "sjc = \"../data/data from 2019 teams/cases.json\"\n",
    "\n",
    "sjc_data_as_file = open(newest_sjc)\n",
    "sjc_new = json.load(sjc_data_as_file)\n",
    "\n",
    "sjc_data_as_file = open(sjc)\n",
    "sjc = json.load(sjc_data_as_file)\n",
    "\n",
    "newest_appeals_court = \"../data/new data w right fields/appeal_mass_new_3_21.json\"\n",
    "appeals_court_data_as_file = open(newest_appeals_court)\n",
    "appeals_court_new = json.load(appeals_court_data_as_file)\n",
    "\n",
    "appeals_court = \"../data/data from 2019 teams/cases_appeals.json\"\n",
    "appeals_court_data_as_file = open(appeals_court)\n",
    "appeals_court= json.load(appeals_court_data_as_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing everything to pandas df bc easier\n",
    "sjc_df = pd.DataFrame(sjc)\n",
    "sjc_df['text']= sjc_df['text'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
    "sjc_n_df = pd.DataFrame(sjc_new)\n",
    "sjc_n_df['text']= sjc_n_df['text'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
    "ac_df = pd.DataFrame(appeals_court )\n",
    "ac_df['text']= ac_df['text'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))\n",
    "ac_n_df = pd.DataFrame(appeals_court_new)\n",
    "ac_n_df = ac_n_df.rename(columns={'text:': 'text'})\n",
    "ac_n_df['text']= ac_n_df['text'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getting all of the text together to vectorize and to figure out similar terms to affirm and reject\n",
    "\n",
    "text = pd.DataFrame(sjc_df[\"text\"])\n",
    "\n",
    "a = pd.DataFrame(ac_n_df[\"text\"])\n",
    "text = text.append(a)\n",
    "\n",
    "b = pd.DataFrame(ac_df[\"text\"])\n",
    "c =  pd.DataFrame(sjc_n_df[\"text\"])\n",
    "text = text.append(b)\n",
    "text = text.append(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "vec =text[\"text\"].tolist()\n",
    "for v in range(len(vec)): \n",
    "    vec[v] = re.sub('([.,!?()])', r' \\1 ', vec[v])\n",
    "    vec[v] = re.sub('\\s{2,}', ' ', vec[v])\n",
    "\n",
    "\n",
    "tokenized = [word_tokenize(i) for i in vec]\n",
    "w2vmodel = gensim.models.Word2Vec(tokenized , size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ref = ['reverse']\n",
    "w2vmodel.wv.most_similar(positive =ref, topn = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aff = ['affirm']\n",
    "w2vmodel.wv.most_similar(positive =aff, topn = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the amount of each terms \n",
    "a = w2vmodel.wv.vocab[\"affirm\"]\n",
    "r = w2vmodel.wv.vocab[\"reverse\"]\n",
    "print(\"affirm count\",  a.count)\n",
    "print(\"reverse count\" , r.count)\n",
    "print(\"total cases\" , text.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words only in cases that have revert and only affirm\n",
    "#train word2vec with differen gram (what words are more likely to appear how many places away)\n",
    "#(how far is informant away)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenized, stop_words='english')\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "pd_matrix = pd.DataFrame(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
