---
title: "Final deliverable"
author: "Nicola Kriefall"
date: "May 1st, 2020"
output: html_document
---

## Overview 

For my project, I am analyzing bacterial microbiomes hosted by reef-building coral species *Siderastrea siderea* in the Florida Keys. Tissue samples of this coral were collected at 6 sites throughout the Florida Keys in 2015, 2017 (just after Hurricane Irma), and 2018. I will be determining whether we can see an impact of Hurricane Irma in disrupted microbial networks at the Irma timepoint. 

## Data collection details

A genetic marker of the bacterial genome called the 16S region was amplified & sequenced for every collected coral sample. This marker acts as a 'barcode' to identify what types of bacteria are present in each coral sample. The 16S sequencing data collected from my samples were analyzed following [Callahan et al., 2016](https://www.nature.com/articles/nmeth.3869). The specific scripts I used are available [here](https://github.com/Nicfall/florida_irma/tree/master/fl16s). Using these scripts, the raw sequencing data were converted to a counts table, where the rows are the coral samples and the columns are bacterial species, containing counts of each appearance of the bacteria in each sample. See below for an example:

## Workflow

### Reading in data

Data is organized with coral samples in rows, and the families of bacteria the corals host in columns. The data are counts of the number of sequence reads that matched each family of bacteria. I have three data frames, one for each of the three sampling years (2015, 2017, 2018).

```{r data}
data_15 <- read.csv("seq.glom.15.no0.csv",row.names=1) #from 2015
data_17 <- read.csv("seq.glom.17.no0.csv",row.names=1) #from 2017
data_18 <- read.csv("seq.glom.18.no0.csv",row.names=1) #from 2018

head(data_15,n=6L) #rows are coral samples, columns are bacterial families & their counts per sample
```

### Data analysis

Begin by installing & loading necessary packages.

(Overall pipeline & scripts followed is sourced from [here; Layeghifard *et al*., 2018, *Methods in Molecular Biology*](https://www-ncbi-nlm-nih-gov.ezproxy.bu.edu/pubmed/30298259))

```{r packages, message=FALSE}
# Install Required packages 
#install.packages("igraph") 
library("igraph")
#install.packages("qgraph") 
library("qgraph")
#install.packages("MCL")
library("MCL")
#install.packages("vegan")
library("vegan")
# Install SpiecEasi package 
#install.packages("devtools") 
library("devtools") 
#install_github("zdk123/SpiecEasi")
library("SpiecEasi")
library("ggplot2")
library("ggpubr")
```

Some data cleaning: authors recommend removing samples with less than an average of 2 reads per column. 

```{r read in data}
#removing low count families
data_15_2 <- data_15[,-which(colMeans(data_15) <= 2)] #lost 26 families
data_17_2 <- data_17[,-which(colMeans(data_17) <= 2)] #lost 33 families
data_18_2 <- data_18[,-which(colMeans(data_18) <= 2)] #lost 39 families
```

### Building adjacency matrix & network

Building an adjacency matrix of correlations of the abundances of bacterial families, one per sampling year. 

```{r build network}
# SparCC network
sparcc.matrix_15 <- sparcc(data_15_2)
sparcc.matrix_17 <- sparcc(data_17_2)
sparcc.matrix_18 <- sparcc(data_18_2)

sparcc.cutoff <- 0.3
sparcc.adj_15 <- ifelse(abs(sparcc.matrix_15$Cor) >= sparcc.cutoff, 1, 0)
sparcc.adj_17 <- ifelse(abs(sparcc.matrix_17$Cor) >= sparcc.cutoff, 1, 0)
sparcc.adj_18 <- ifelse(abs(sparcc.matrix_18$Cor) >= sparcc.cutoff, 1, 0)

# Add family names to rows and columns
rownames(sparcc.adj_15) <- colnames(data_15_2) 
colnames(sparcc.adj_15) <- colnames(data_15_2) 

rownames(sparcc.adj_17) <- colnames(data_17_2) 
colnames(sparcc.adj_17) <- colnames(data_17_2) 

rownames(sparcc.adj_18) <- colnames(data_18_2) 
colnames(sparcc.adj_18) <- colnames(data_18_2) 

# Build network from adjacency
sparcc.net_15 <- graph.adjacency(sparcc.adj_15,
                              mode = "undirected",
                              diag = FALSE)
sparcc.net_17 <- graph.adjacency(sparcc.adj_17,
                              mode = "undirected",
                              diag = FALSE)
sparcc.net_18 <- graph.adjacency(sparcc.adj_18,
                              mode = "undirected",
                              diag = FALSE)
```

### Network metrics

```{r}
# Use sparcc.net for the rest of the method 
net_15 <- sparcc.net_15
net_17 <- sparcc.net_17
net_18 <- sparcc.net_18
# Hub detection
net_15.cn <- closeness(net_15)
net_15.bn <- betweenness(net_15) 
net_15.pr <- page_rank(net_15)$vector 
net_15.hs <- hub_score(net_15)$vector
net_15.hs.sort <- sort(net_15.hs, decreasing = TRUE)
net_15.hs.top5 <- head(net_15.hs.sort, n = 5)

net_17.cn <- closeness(net_17)
net_17.bn <- betweenness(net_17) 
net_17.pr <- page_rank(net_17)$vector 
net_17.hs <- hub_score(net_17)$vector
net_17.hs.sort <- sort(net_17.hs, decreasing = TRUE)
net_17.hs.top5 <- head(net_17.hs.sort, n = 5)

net_18.cn <- closeness(net_18)
net_18.bn <- betweenness(net_18) 
net_18.pr <- page_rank(net_18)$vector 
net_18.hs <- hub_score(net_18)$vector
net_18.hs.sort <- sort(net_18.hs, decreasing = TRUE)
net_18.hs.top5 <- head(net_18.hs.sort, n = 5)
```

### Eigenvector centrality

```{r network metrics 1}
eigen.15 <- eigen_centrality(net_15)
eigen.15.tosort <- data.frame(eigen.15[["vector"]])
eigen.15.tosort$vector <- eigen.15.tosort$eigen.15...vector...
eigen.15.sorted <- eigen.15.tosort[order(-eigen.15.tosort$vector),]
eigen.15.sorted$sqs <- rownames(eigen.15.sorted)
eigen.15.top10 <- eigen.15.sorted[1:10,]
sqs_order <- eigen.15.top10$sqs
eigen.15.top10$sqs <- factor(eigen.15.top10$sqs,levels=sqs_order)

eigen.17 <- eigen_centrality(net_17)
eigen.17.tosort <- data.frame(eigen.17[["vector"]])
eigen.17.tosort$vector <- eigen.17.tosort$eigen.17...vector...
eigen.17.sorted <- eigen.17.tosort[order(-eigen.17.tosort$vector),]
eigen.17.sorted$sqs <- rownames(eigen.17.sorted)
eigen.17.top10 <- eigen.17.sorted[1:10,]
sqs_order <- eigen.17.top10$sqs
eigen.17.top10$sqs <- factor(eigen.17.top10$sqs,levels=sqs_order)

eigen.18 <- eigen_centrality(net_18)
eigen.18.tosort <- data.frame(eigen.18[["vector"]])
eigen.18.tosort$vector <- eigen.18.tosort$eigen.18...vector...
eigen.18.sorted <- eigen.18.tosort[order(-eigen.18.tosort$vector),]
eigen.18.sorted$sqs <- rownames(eigen.18.sorted)
eigen.18.top10 <- eigen.18.sorted[1:10,]
sqs_order <- eigen.18.top10$sqs
eigen.18.top10$sqs <- factor(eigen.18.top10$sqs,levels=sqs_order)

ggplot(eigen.15.top10,aes(x=sqs,y=vector))+
  geom_bar(stat="identity")+
  theme(axis.text.x=element_text(angle=90,hjust=1))+
  ylab("Eigenvector centrality")+
  xlab("Bacterial sequence")+
  ggtitle("2015 & 2016 (pre-Irma)")+
  geom_hline(yintercept=0.7885872,color="#781C6D")

ggplot(eigen.17.top10,aes(x=sqs,y=vector))+
  geom_bar(stat="identity")+
  theme(axis.text.x=element_text(angle=90,hjust=1))+
  ylab("Eigenvector centrality")+
  xlab("Bacterial sequence")+
  ggtitle("2017 (Irma)")+
  geom_hline(yintercept=0.7885872,color="#781C6D")+
  geom_hline(yintercept=0.7140983,color="#AE305C")

ggplot(eigen.18.top10,aes(x=sqs,y=vector))+
  geom_bar(stat="identity")+
  theme(axis.text.x=element_text(angle=90,hjust=1))+
  ylab("Eigenvector centrality")+
  xlab("Bacterial sequence")+
  ggtitle("2018 (post-Irma)")+
  geom_hline(yintercept=0.7140983,color="#AE305C")
```

#### Analysis

Lots of cool things here: 
I put a solid line at the lowest eigenvector centrality of the 2015 data and a dashed line at the lowest of the 2018 data, and put both of these lines on the 2017 data to show that most of the top 10 most central nodes in 2017 didn't come anywhere near as close to the other years in this metric. There are 2 bacterial families that show up as most central between all three time points. However, **five** families are shared between pre-Irma & post-Irma, while only 1 family is shared between pre-Irma & Irma and another 1 family shared between Irma & post-Irma. In other words, Irma had the most unique central nodes, while the pre & post time points are more similar in their top 10 compositions. This could all be evidence of a disrupted network due to Hurricane Irma.

### Betweenness centrality

```{r network networks 2}
btwn.15.tosort <- data.frame(betweenness(net_15))
btwn.15.tosort$vector <- btwn.15.tosort$betweenness.net_15.
btwn.15.sorted <- btwn.15.tosort[order(-btwn.15.tosort$vector),]
btwn.15.sorted$sqs <- rownames(btwn.15.sorted)
btwn.15.top10 <- btwn.15.sorted[1:10,]
sqs_order <- btwn.15.top10$sqs
btwn.15.top10$sqs <- factor(btwn.15.top10$sqs,levels=sqs_order)

btwn.17.tosort <- data.frame(betweenness(net_17))
btwn.17.tosort$vector <- btwn.17.tosort$betweenness.net_17.
btwn.17.sorted <- btwn.17.tosort[order(-btwn.17.tosort$vector),]
btwn.17.sorted$sqs <- rownames(btwn.17.sorted)
btwn.17.top10 <- btwn.17.sorted[1:10,]
sqs_order <- btwn.17.top10$sqs
btwn.17.top10$sqs <- factor(btwn.17.top10$sqs,levels=sqs_order)

btwn.18.tosort <- data.frame(betweenness(net_18))
btwn.18.tosort$vector <- btwn.18.tosort$betweenness.net_18.
btwn.18.sorted <- btwn.18.tosort[order(-btwn.18.tosort$vector),]
btwn.18.sorted$sqs <- rownames(btwn.18.sorted)
btwn.18.top10 <- btwn.18.sorted[1:10,]
sqs_order <- btwn.18.top10$sqs
btwn.18.top10$sqs <- factor(btwn.18.top10$sqs,levels=sqs_order)

ggplot(btwn.15.top10,aes(x=sqs,y=vector))+
  geom_bar(stat="identity")+
  theme(axis.text.x=element_text(angle=90,hjust=1))+
  ylab("Betweenness centrality")+
  xlab("Bacterial sequence")+
  ggtitle("2015 & 2016 (pre-Irma)")

ggplot(btwn.17.top10,aes(x=sqs,y=vector))+
  geom_bar(stat="identity")+
  theme(axis.text.x=element_text(angle=90,hjust=1))+
  ylab("Betweenness centrality")+
  xlab("Bacterial sequence")+
  ggtitle("2017 (Irma)")

ggplot(btwn.18.top10,aes(x=sqs,y=vector))+
  geom_bar(stat="identity")+
  theme(axis.text.x=element_text(angle=90,hjust=1))+
  ylab("Betweenness centrality")+
  xlab("Bacterial sequence")+
  ggtitle("2018 (post-Irma)")
```

#### Analysis

Again, these values look more similar between pre & post-Irma time points than either of these with the Irma time point. 

### Visualizing network from adjacency matrix

```{r}
#devtools::install_github("briatte/ggnet")
library(ggnet)

ggnet2(sparcc.adj_15,size="degree",size.cut=TRUE,alpha=0.5,color="#781C6D")+
  ggtitle('Pre-Irma (2015)')
ggnet2(sparcc.adj_17,size="degree",size.cut=TRUE,alpha=0.5,color="#F8850F")+
  ggtitle('Irma (2017)')
ggnet2(sparcc.adj_18,size="degree",size.cut=TRUE,alpha=0.5,color="#AE305C")+
  ggtitle('Recovery? (2018)')
```

### Looking at results

It looks like there are a lot less connections in the microbial networks in 2017 (Hurricane Irma timepoint; shown by the disconnected nodes off to the side that did not correlate with other nodes) which is exactly what I was hypothesizing (that the Irma timepoint would be the most disrupted). In addition, there appears to be more dispersion in the network during Irma (more spread out nodes rather than the huge hubs we see in the other two time points). You can clearly see the quantiles of degree ranges in the legends are the smallest during Irma (highest degree range was 14-62 during Irma as compared to 31-90 and 38-84 pre & post-Irma, respectively). PR oIf I had more time, I would figure out how to do the statistics on these analyses. 

